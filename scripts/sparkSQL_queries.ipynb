{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "\n",
    "# MongoDB connection details\n",
    "MONGO_URI = \"mongodb+srv://<username>:<password>.iinrc.mongodb.net/?retryWrites=true&w=majority&appName=makertdata\"\n",
    "MONGO_DB = \"stock_data\"\n",
    "MONGO_COLLECTION = \"aapl_news\"\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = pymongo.MongoClient(MONGO_URI)\n",
    "db = client[MONGO_DB]\n",
    "collection = db[MONGO_COLLECTION]\n",
    "\n",
    "# Count documents in the collection\n",
    "collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/07 15:11:42 WARN Utils: Your hostname, Daniels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)\n",
      "25/03/07 15:11:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/danielmendoza/anaconda3/envs/DistributedComputing/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/danielmendoza/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/danielmendoza/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-74b2781c-61f7-4947-bd2b-b6a27966f62f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (70ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (33ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (67ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (78ms)\n",
      ":: resolution report :: resolve 633ms :: artifacts dl 284ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-74b2781c-61f7-4947-bd2b-b6a27966f62f\n",
      "\tconfs: [default]\n",
      "\t4 artifacts copied, 0 already retrieved (2728kB/15ms)\n",
      "25/03/07 15:11:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+-----------------------------------------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_id                       |ticker_sentiment                               |time_published |title                                                                                                                                                                                                                                     |\n",
      "+--------------------------+-----------------------------------------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{67b8d4f5be7232552cbd1faa}|[{AAPL, 0.124971, 0.007351, Neutral}]          |20250221T141018|SPY: How High or Low Could It Potentially Go Today? - Microsoft  ( NASDAQ:MSFT )                                                                                                                                                          |\n",
      "|{67b8d4f5be7232552cbd1fab}|[{AAPL, 0.090588, -0.09278, Neutral}]          |20250220T142436|How To Trade SPY, QQQ, AAPL, MSFT, NVDA, GOOGL, META, TSLA Today                                                                                                                                                                          |\n",
      "|{67b8d4f5be7232552cbd1fac}|[{AAPL, 0.234684, 0.084639, Neutral}]          |20250220T111019|Bitcoin-Friendly Bank Stock Held By Warren Buffett And Cathie Wood Has Crushed Their Top Holdings Apple And Tesla In 2025 - Grayscale Bitcoin Mini Trust  ( BTC )  Common units of fractional undivided beneficial interest  ( ARCA:BTC ) |\n",
      "|{67b8d4f5be7232552cbd1fad}|[{AAPL, 0.246999, 0.0, Neutral}]               |20250220T085609|Trump's Goal Is To 'Abolish' IRS And 'Let All The Outsiders Pay,' Says Newly Confirmed Commerce Secretary Howard Lutnick - Apple  ( NASDAQ:AAPL ) , Boeing  ( NYSE:BA )                                                                   |\n",
      "|{67b8d4f5be7232552cbd1fae}|[{AAPL, 0.360752, -0.221867, Somewhat-Bearish}]|20250219T195819|EXCLUSIVE: Apple, Nvidia, Tesla Or Boeing? Which Company Faces the Biggest Risk From Trump's China Tariffs - 35% Say This Stock - Apple  ( NASDAQ:AAPL ) , Boeing  ( NYSE:BA )                                                            |\n",
      "+--------------------------+-----------------------------------------------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DataFrame Schema:\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- ticker_sentiment: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- ticker: string (nullable = true)\n",
      " |    |    |-- relevance_score: string (nullable = true)\n",
      " |    |    |-- ticker_sentiment_score: string (nullable = true)\n",
      " |    |    |-- ticker_sentiment_label: string (nullable = true)\n",
      " |-- time_published: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "Total number of records: 94\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with MongoDB connector - using version 3.0.1 which is more widely available\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Apple Sentiment Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# MongoDB connection details\n",
    "mongodb_uri = \"mongodb+srv://<username>:<password>@makertdata.iinrc.mongodb.net/?retryWrites=true&w=majority\"\n",
    "\n",
    "# Load data from MongoDB - using the older MongoDB connector syntax\n",
    "news_df = spark.read \\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"uri\", mongodb_uri) \\\n",
    "    .load()\n",
    "\n",
    "# Register as a temp view for SQL queries\n",
    "news_df.createOrReplaceTempView(\"aapl_news\")\n",
    "\n",
    "# Display the first few rows to verify data is loaded correctly\n",
    "news_df.show(5, truncate=False)\n",
    "\n",
    "# Get basic info about the dataframe\n",
    "print(\"DataFrame Schema:\")\n",
    "news_df.printSchema()\n",
    "\n",
    "print(f\"Total number of records: {news_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark SQL execution time: 0.5489 seconds\n",
      "Sentiment Analysis Results:\n",
      "+----------+---------------------+-------------+\n",
      "|date      |avg_sentiment        |article_count|\n",
      "+----------+---------------------+-------------+\n",
      "|2025-01-29|0.08027400000000001  |3            |\n",
      "|2025-01-30|0.1187685            |2            |\n",
      "|2025-01-31|-0.028153333333333325|3            |\n",
      "|2025-02-03|-0.14467             |1            |\n",
      "|2025-02-04|0.08738233333333334  |3            |\n",
      "|2025-02-05|0.084617             |1            |\n",
      "|2025-02-06|-0.051376            |1            |\n",
      "|2025-02-10|0.17859125           |4            |\n",
      "|2025-02-12|0.0457248            |5            |\n",
      "|2025-02-13|0.221359             |1            |\n",
      "|2025-02-14|0.030186199999999996 |5            |\n",
      "|2025-02-15|0.152276             |3            |\n",
      "|2025-02-16|0.18065033333333333  |3            |\n",
      "|2025-02-17|0.10014              |2            |\n",
      "|2025-02-18|0.12344              |4            |\n",
      "|2025-02-19|0.135929             |13           |\n",
      "|2025-02-20|0.15929923809523808  |21           |\n",
      "|2025-02-21|0.17185505263157896  |19           |\n",
      "+----------+---------------------+-------------+\n",
      "\n",
      "Results as Pandas DataFrame:\n",
      "          date  avg_sentiment  article_count\n",
      "0   2025-01-29       0.080274              3\n",
      "1   2025-01-30       0.118768              2\n",
      "2   2025-01-31      -0.028153              3\n",
      "3   2025-02-03      -0.144670              1\n",
      "4   2025-02-04       0.087382              3\n",
      "5   2025-02-05       0.084617              1\n",
      "6   2025-02-06      -0.051376              1\n",
      "7   2025-02-10       0.178591              4\n",
      "8   2025-02-12       0.045725              5\n",
      "9   2025-02-13       0.221359              1\n",
      "10  2025-02-14       0.030186              5\n",
      "11  2025-02-15       0.152276              3\n",
      "12  2025-02-16       0.180650              3\n",
      "13  2025-02-17       0.100140              2\n",
      "14  2025-02-18       0.123440              4\n",
      "15  2025-02-19       0.135929             13\n",
      "16  2025-02-20       0.159299             21\n",
      "17  2025-02-21       0.171855             19\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Time the Spark SQL query execution\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the sentiment analysis with SparkSQL\n",
    "sentiment_analysis = spark.sql(\"\"\"\n",
    "    WITH sentiment_data AS (\n",
    "        SELECT \n",
    "            explode(ticker_sentiment) as ticker_info,\n",
    "            time_published\n",
    "        FROM aapl_news\n",
    "    )\n",
    "    \n",
    "    SELECT \n",
    "        CONCAT(SUBSTRING(time_published, 1, 4), '-', \n",
    "               SUBSTRING(time_published, 5, 2), '-', \n",
    "               SUBSTRING(time_published, 7, 2)) as date,\n",
    "        AVG(CAST(ticker_info.ticker_sentiment_score AS DOUBLE)) as avg_sentiment,\n",
    "        COUNT(*) as article_count\n",
    "    FROM sentiment_data\n",
    "    WHERE ticker_info.ticker = 'AAPL'\n",
    "    GROUP BY CONCAT(SUBSTRING(time_published, 1, 4), '-', \n",
    "                   SUBSTRING(time_published, 5, 2), '-', \n",
    "                   SUBSTRING(time_published, 7, 2))\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "\n",
    "# Calculate execution time\n",
    "spark_execution_time = time.time() - start_time\n",
    "print(f\"Spark SQL execution time: {spark_execution_time:.4f} seconds\")\n",
    "\n",
    "# Show the results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "sentiment_analysis.show(20, False)\n",
    "\n",
    "# Convert to Pandas dataframe for easier manipulation (optional)\n",
    "pandas_df = sentiment_analysis.toPandas()\n",
    "print(\"Results as Pandas DataFrame:\")\n",
    "print(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB execution time: 0.8236 seconds\n",
      "{'_id': '2025-01-29', 'avgSentiment': 0.08027400000000001, 'articleCount': 3}\n",
      "{'_id': '2025-01-30', 'avgSentiment': 0.1187685, 'articleCount': 2}\n",
      "{'_id': '2025-01-31', 'avgSentiment': -0.028153333333333325, 'articleCount': 3}\n",
      "{'_id': '2025-02-03', 'avgSentiment': -0.14467, 'articleCount': 1}\n",
      "{'_id': '2025-02-04', 'avgSentiment': 0.08738233333333334, 'articleCount': 3}\n",
      "{'_id': '2025-02-05', 'avgSentiment': 0.084617, 'articleCount': 1}\n",
      "{'_id': '2025-02-06', 'avgSentiment': -0.051376, 'articleCount': 1}\n",
      "{'_id': '2025-02-10', 'avgSentiment': 0.17859125, 'articleCount': 4}\n",
      "{'_id': '2025-02-12', 'avgSentiment': 0.0457248, 'articleCount': 5}\n",
      "{'_id': '2025-02-13', 'avgSentiment': 0.221359, 'articleCount': 1}\n",
      "{'_id': '2025-02-14', 'avgSentiment': 0.030186199999999996, 'articleCount': 5}\n",
      "{'_id': '2025-02-15', 'avgSentiment': 0.152276, 'articleCount': 3}\n",
      "{'_id': '2025-02-16', 'avgSentiment': 0.18065033333333333, 'articleCount': 3}\n",
      "{'_id': '2025-02-17', 'avgSentiment': 0.10014, 'articleCount': 2}\n",
      "{'_id': '2025-02-18', 'avgSentiment': 0.12344, 'articleCount': 4}\n",
      "{'_id': '2025-02-19', 'avgSentiment': 0.135929, 'articleCount': 13}\n",
      "{'_id': '2025-02-20', 'avgSentiment': 0.1592992380952381, 'articleCount': 21}\n",
      "{'_id': '2025-02-21', 'avgSentiment': 0.17185505263157894, 'articleCount': 19}\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import time\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb+srv://<username>:<password>@makertdata.iinrc.mongodb.net/\")\n",
    "db = client[\"stock_data\"]\n",
    "collection = db[\"aapl_news\"]\n",
    "\n",
    "# Time the MongoDB aggregation query\n",
    "start_time = time.time()\n",
    "\n",
    "# MongoDB aggregation pipeline (same as the one you used)\n",
    "pipeline = [\n",
    "  { \"$unwind\": \"$ticker_sentiment\" },\n",
    "  { \"$match\": { \"ticker_sentiment.ticker\": \"AAPL\" } },\n",
    "  { \"$addFields\": {\n",
    "      \"date\": {\n",
    "        \"$concat\": [\n",
    "          { \"$substr\": [\"$time_published\", 0, 4] }, \"-\",\n",
    "          { \"$substr\": [\"$time_published\", 4, 2] }, \"-\",\n",
    "          { \"$substr\": [\"$time_published\", 6, 2] }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  { \"$group\": {\n",
    "      \"_id\": \"$date\",\n",
    "      \"avgSentiment\": { \"$avg\": { \"$toDouble\": \"$ticker_sentiment.ticker_sentiment_score\" } },\n",
    "      \"articleCount\": { \"$sum\": 1 }\n",
    "    }\n",
    "  },\n",
    "  { \"$sort\": { \"_id\": 1 } }\n",
    "]\n",
    "\n",
    "# Execute the query\n",
    "result = list(collection.aggregate(pipeline))\n",
    "\n",
    "# Calculate execution time\n",
    "mongo_execution_time = time.time() - start_time\n",
    "print(f\"MongoDB execution time: {mongo_execution_time:.4f} seconds\")\n",
    "\n",
    "# Print results for comparison\n",
    "for doc in result:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MongoDB execution time: 0.8502 seconds\n",
      "MongoDB Results:\n",
      "Symbol: TSLA, Avg Close: $338.47, Avg Volatility: 4.75%, Total Volume: 8,689,641,720.0\n",
      "Symbol: AAPL, Avg Close: $237.72, Avg Volatility: 2.25%, Total Volume: 5,427,330,811.0\n",
      "Symbol: GOOGL, Avg Close: $193.84, Avg Volatility: 2.13%, Total Volume: 2,660,553,958.0\n",
      "Symbol: MSFT, Avg Close: $424.63, Avg Volatility: 1.72%, Total Volume: 2,138,416,953.0\n",
      "Symbol: BA, Avg Close: $173.01, Avg Volatility: 3.32%, Total Volume: 175,229,284.0\n",
      "Symbol: GBTC, Avg Close: $79.65, Avg Volatility: 3.29%, Total Volume: 64,187,835.0\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import time\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(\"mongodb+srv://<username>:<password>@makertdata.iinrc.mongodb.net/\")\n",
    "db = client[\"stock_data\"]\n",
    "collection = db[\"market_prices_master\"]  # Use your actual collection name\n",
    "\n",
    "# Time the MongoDB aggregation query\n",
    "start_time = time.time()\n",
    "\n",
    "# MongoDB aggregation pipeline for price statistics by symbol\n",
    "pipeline = [\n",
    "  # Group by symbol\n",
    "  { \"$group\": {\n",
    "      \"_id\": \"$symbol\",\n",
    "      \"count\": { \"$sum\": 1 },\n",
    "      \"avg_close\": { \"$avg\": \"$close\" },\n",
    "      \"min_close\": { \"$min\": \"$close\" },\n",
    "      \"max_close\": { \"$max\": \"$close\" },\n",
    "      \"avg_volume\": { \"$avg\": \"$volume\" },\n",
    "      \"total_volume\": { \"$sum\": \"$volume\" },\n",
    "      # Calculate price volatility stats\n",
    "      \"avg_price_range\": { \n",
    "        \"$avg\": { \n",
    "          \"$subtract\": [\"$high\", \"$low\"] \n",
    "        }\n",
    "      },\n",
    "      \"max_price_range\": {\n",
    "        \"$max\": { \n",
    "          \"$subtract\": [\"$high\", \"$low\"] \n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \n",
    "  # Calculate price range as percentage of average closing price\n",
    "  { \"$addFields\": {\n",
    "      \"avg_volatility_pct\": {\n",
    "        \"$multiply\": [\n",
    "          { \"$divide\": [\"$avg_price_range\", \"$avg_close\"] },\n",
    "          100\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \n",
    "  # Sort by trading volume (most active stocks first)\n",
    "  { \"$sort\": { \"total_volume\": -1 } },\n",
    "  \n",
    "  # Limit to top 10 stocks by volume\n",
    "  { \"$limit\": 10 }\n",
    "]\n",
    "\n",
    "# Execute the query\n",
    "result = list(collection.aggregate(pipeline))\n",
    "\n",
    "# Calculate execution time\n",
    "mongo_execution_time = time.time() - start_time\n",
    "print(f\"MongoDB execution time: {mongo_execution_time:.4f} seconds\")\n",
    "\n",
    "# Print results for comparison\n",
    "print(\"MongoDB Results:\")\n",
    "for doc in result:\n",
    "    print(f\"Symbol: {doc['_id']}, Avg Close: ${doc['avg_close']:.2f}, Avg Volatility: {doc['avg_volatility_pct']:.2f}%, Total Volume: {doc['total_volume']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/07 15:33:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark SQL execution time: 0.0950 seconds\n",
      "Spark SQL Results:\n",
      "+------+------------+------------------+---------+---------+--------------------+-------------+------------------+------------------+------------------+\n",
      "|symbol|trading_days|avg_close         |min_close|max_close|avg_volume          |total_volume |avg_price_range   |max_price_range   |avg_volatility_pct|\n",
      "+------+------------+------------------+---------+---------+--------------------+-------------+------------------+------------------+------------------+\n",
      "|TSLA  |100         |338.4703          |213.65   |479.86   |8.68964172E7        |8.68964172E9 |16.067099000000002|61.5299           |4.746974549908812 |\n",
      "|AAPL  |102         |237.72049019607837|222.64   |259.02   |5.320912559803922E7 |5.427330811E9|5.347441176470589 |13.75             |2.2494658209983807|\n",
      "|GOOGL |99          |193.84404040404038|183.61   |206.38   |2.6874282404040404E7|2.660553958E9|4.124191919191918 |9.259999999999991 |2.127582519738871 |\n",
      "|MSFT  |99          |424.63111111111107|408.43   |447.2    |2.1600171242424242E7|2.138416953E9|7.3247656565656545|17.0              |1.7249715022997973|\n",
      "|BA    |20          |173.01            |166.2    |179.53   |8761464.2           |1.75229284E8 |5.7485            |14.45999999999998 |3.3226403098086816|\n",
      "|GBTC  |20          |79.64850000000003 |74.16    |84.12    |3209391.75          |6.4187835E7  |2.6170500000000003|3.8400000000000034|3.285749260814704 |\n",
      "+------+------------+------------------+---------+---------+--------------------+-------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create Spark session with MongoDB connector\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stock Price Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# MongoDB connection details\n",
    "mongodb_uri = \"mongodb+srv://<username>:<password>.iinrc.mongodb.net/stock_data.market_prices_master\"\n",
    "\n",
    "# Load data from MongoDB\n",
    "prices_df = spark.read \\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"uri\", mongodb_uri) \\\n",
    "    .load()\n",
    "\n",
    "# Register as a temp view for SQL queries\n",
    "prices_df.createOrReplaceTempView(\"market_prices\")\n",
    "\n",
    "# Time the Spark SQL query execution\n",
    "start_time = time.time()\n",
    "\n",
    "# Run the price statistics analysis with SparkSQL\n",
    "price_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        symbol,\n",
    "        COUNT(*) as trading_days,\n",
    "        AVG(close) as avg_close,\n",
    "        MIN(close) as min_close,\n",
    "        MAX(close) as max_close,\n",
    "        AVG(volume) as avg_volume,\n",
    "        SUM(volume) as total_volume,\n",
    "        AVG(high - low) as avg_price_range,\n",
    "        MAX(high - low) as max_price_range,\n",
    "        (AVG(high - low) / AVG(close)) * 100 as avg_volatility_pct\n",
    "    FROM market_prices\n",
    "    GROUP BY symbol\n",
    "    ORDER BY total_volume DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "# Calculate execution time\n",
    "spark_execution_time = time.time() - start_time\n",
    "print(f\"Spark SQL execution time: {spark_execution_time:.4f} seconds\")\n",
    "\n",
    "# Show the results\n",
    "print(\"Spark SQL Results:\")\n",
    "price_analysis.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DistributedComputing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
